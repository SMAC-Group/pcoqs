{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff718ea-0eb7-4630-ad66-f17d497c8dce",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29981a95-8340-432e-927e-9fc56c1cb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys, inspect\n",
    "import torchvision as tv\n",
    "import torchvision.models as models\n",
    "import argparse\n",
    "import time\n",
    "from scipy.stats import binom\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import binom, beta\n",
    "from scipy.special import softmax\n",
    "import pdb\n",
    "import glob\n",
    "import seaborn as sns\n",
    "dirname = str(pathlib.Path().absolute())\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "from matplotlib.font_manager import FontProperties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3a07b-cac2-4046-ac1d-c488017a165e",
   "metadata": {},
   "source": [
    "### Helping function for PCOSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f1513-8b32-47aa-96d3-0d3faaffedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRC(range_bounds, D, sigma):\n",
    "    \"\"\"\n",
    "    Noisy Range Count for float values with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    range_bounds (tuple): A tuple (a, b) representing the range [a, b].\n",
    "    D (list): The sorted dataset.\n",
    "    sigma (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    int: The noisy count of elements in the range [a, b].\n",
    "    \"\"\"\n",
    "    a, b = range_bounds\n",
    "    count = sum(1 for z in D if a <= z <= b)\n",
    "    noise = np.random.normal(0, sigma)\n",
    "    noisy_count = count + noise\n",
    "    return max(0, int(np.floor(noisy_count)))  # Ensure non-negative count\n",
    "\n",
    "def PrivQuant(D, alpha, rho, seed, lower_bound=0, upper_bound=1, delta=1e-10):\n",
    "    \"\"\"\n",
    "    Differentially Private Quantile Approximation Algorithm without integer conversion.\n",
    "\n",
    "    Parameters:\n",
    "    D (list): The sorted dataset.\n",
    "    alpha (float): The quantile level (e.g., 0.5 for median).\n",
    "    rho (float): The privacy parameter (smaller = more private).\n",
    "    lower_bound (float): Lower bound of the search space.\n",
    "    upper_bound (float): Upper bound of the search space.\n",
    "    delta (float): Small positive value to ensure convergence.\n",
    "\n",
    "    Returns:\n",
    "    float: A differentially private approximation of the quantile x_{(m)}.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n = len(D)\n",
    "    max_iterations = int(np.ceil(np.log2((upper_bound - lower_bound) / delta)))\n",
    "    sigma = np.sqrt(max_iterations / (2 * rho)) # Noise scale for Gaussian mechanism\n",
    "    m = int(np.ceil((1 - alpha) * (n + 1)))\n",
    "\n",
    "    left, right = lower_bound, upper_bound\n",
    "    random.seed(seed)\n",
    "    for i in range(max_iterations):\n",
    "        mid = (left + right) / 2\n",
    "        c = NoisyRC((lower_bound, mid), D, sigma)\n",
    "        \n",
    "        if c < m:\n",
    "            left = mid + delta\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return np.round((left + right) / 2, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462428b2-9f22-43fe-b9cd-d6f011945a26",
   "metadata": {},
   "source": [
    "### Helping function Lap_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6fe35-e482-4b3c-9898-a2533ad7f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_quantile_noisy_hist(x, q, epsilon, seed, bins=50, domain=(0.0, 1.0), rng=None):\n",
    "    \"\"\"\n",
    "    Differentially private quantile using a Laplace-noised histogram (ε-DP).\n",
    "\n",
    "    Args:\n",
    "        x (array-like): data vector (numeric).\n",
    "        q (float): desired quantile in (0,1).\n",
    "        epsilon (float): privacy budget for the entire histogram.\n",
    "        domain (tuple): (lo, hi) public bounds for clipping/binning.\n",
    "        bins (int): number of fixed, public bins.\n",
    "        rng: np.random.Generator (optional).\n",
    "\n",
    "    Returns:\n",
    "        float: DP quantile estimate (can lie between data points).\n",
    "\n",
    "    Privacy & assumptions:\n",
    "        - Data are clipped to the public domain (lo, hi).\n",
    "        - Build a fixed-bin histogram, add Lap(1/ε) noise to each bin count.\n",
    "        - Because each record contributes to exactly one bin, releasing\n",
    "          the full noisy histogram is ε-DP under add/remove adjacency.\n",
    "        - Quantile is computed from the noisy cumulative counts.\n",
    "\n",
    "    Notes:\n",
    "        - Works best if a reasonable public domain is known.\n",
    "        - For stability, negative noisy counts are floored at 0.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty.\")\n",
    "    if not (0 < q < 1):\n",
    "        raise ValueError(\"q must be in (0,1).\")\n",
    "    if epsilon <= 0:\n",
    "        raise ValueError(\"epsilon must be > 0.\")\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    lo, hi = domain\n",
    "    if not (lo < hi):\n",
    "        raise ValueError(\"domain must satisfy lo < hi.\")\n",
    "\n",
    "    # Clip to public domain\n",
    "    xc = np.clip(x, lo, hi)\n",
    "\n",
    "    # Fixed public bins\n",
    "    edges = np.linspace(lo, hi, bins + 1)\n",
    "    #print(f\"Bins: {edges}\")\n",
    "    counts, _ = np.histogram(xc, bins=edges)\n",
    "    #print(f\"Counts of histogram: {counts}\")\n",
    "\n",
    "    # Laplace noise to each bin (scale = 1/ε)\n",
    "    noise = rng.laplace(loc=0.0, scale=1.0/epsilon, size=bins)\n",
    "    #print(f\"Noise for each bin: {noise}\")\n",
    "    noisy = np.maximum(counts + noise, 0.0)\n",
    "    #print(noisy)\n",
    "\n",
    "    # Cumulative proportion\n",
    "    csum = np.cumsum(noisy)\n",
    "    if csum[-1] <= 0:\n",
    "        # extremely unlikely unless ε is tiny and n is tiny\n",
    "        return float(np.median(xc))\n",
    "\n",
    "    target = q * csum[-1]\n",
    "    j = np.searchsorted(csum, target)  # first bin reaching the target\n",
    "\n",
    "    j = int(np.clip(j, 0, bins - 1))\n",
    "    # Linear interpolation within the bin (simple, uniform-within-bin)\n",
    "    bin_lo, bin_hi = edges[j], edges[j + 1]\n",
    "    prev = csum[j - 1] if j > 0 else 0.0\n",
    "    within = (target - prev) / max(noisy[j], 1e-12)\n",
    "    within = np.clip(within, 0.0, 1.0)\n",
    "    return float(bin_lo + within * (bin_hi - bin_lo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba34b78-0a0f-4072-b27e-a640e83232db",
   "metadata": {},
   "source": [
    "### Helping function for EXPONQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16171fe-7d4b-4cf6-981e-958cb8e09ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_qtilde(n,alpha,gamma,epsilon,m):\n",
    "    qtilde = (n+1)*(1-alpha)/(n*(1-gamma*alpha))+2/(epsilon*n)*np.log(m/(gamma*alpha))\n",
    "    qtilde = min(qtilde, 1-1e-12)\n",
    "    return qtilde\n",
    "\n",
    "def generate_scores(n):\n",
    "    return np.random.uniform(size=(n,))\n",
    "\n",
    "def hist_2_cdf(cumsum, bins, n):\n",
    "    def _cdf(t):\n",
    "        if t > bins[-2]:\n",
    "            return 1.0\n",
    "        elif t < bins[1]:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1-cumsum[np.searchsorted(bins, t)]/n\n",
    "    return _cdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_private_quantile(scores, alpha, epsilon, gamma, bins):\n",
    "    n = scores.shape[0]\n",
    "    epsilon_normed = epsilon*min(alpha, 1-alpha)\n",
    "    # Get the quantile\n",
    "    qtilde = get_qtilde(n, alpha, gamma, epsilon, bins.shape[0])\n",
    "    scores = scores.squeeze()\n",
    "    score_to_bin = np.digitize(scores,bins)\n",
    "    binned_scores = bins[np.minimum(score_to_bin,bins.shape[0]-1)]\n",
    "    w1 = np.digitize(binned_scores, bins)\n",
    "    w2 = np.digitize(binned_scores, bins, right=True)\n",
    "    # Clip bins\n",
    "    w1 = np.maximum(np.minimum(w1,bins.shape[0]-1),0)\n",
    "    w2 = np.maximum(np.minimum(w2,bins.shape[0]-1),0)\n",
    "    lower_mass = np.bincount(w1,minlength=bins.shape[0]).cumsum()/qtilde\n",
    "    upper_mass = (n-np.bincount(w2,minlength=bins.shape[0]).cumsum())/(1-qtilde)\n",
    "    w = np.maximum( lower_mass , upper_mass )\n",
    "    sampling_probabilities = softmax(-(epsilon_normed/2)*w)\n",
    "    # Check\n",
    "    sampling_probabilities = sampling_probabilities/sampling_probabilities.sum()\n",
    "    qhat = np.random.choice(bins,p=sampling_probabilities)\n",
    "    return qhat\n",
    "\n",
    "# Optimal gamma is a root.\n",
    "def get_optimal_gamma(scores,n,alpha,m,epsilon):\n",
    "    a = alpha**2\n",
    "    b = - ( alpha*epsilon*(n+1)*(1-alpha)/2 + 2*alpha )\n",
    "    c = 1\n",
    "    best_q = 1\n",
    "    gamma1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "    gamma2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "\n",
    "    gamma1 = min(max(gamma1,1e-12),1-1e-12)\n",
    "    gamma2 = min(max(gamma2,1e-12),1-1e-12)\n",
    "\n",
    "    bins = np.linspace(0,1,m)\n",
    "\n",
    "    q1 = get_private_quantile(scores, alpha, epsilon, gamma1, bins)\n",
    "    q2 = get_private_quantile(scores, alpha, epsilon, gamma2, bins)\n",
    "\n",
    "    return (gamma1, q1) if q1 < q2 else (gamma2, q2)\n",
    "\n",
    "def get_optimal_gamma_m(n, alpha, epsilon):\n",
    "    candidates_m = np.logspace(4,6,50).astype(int)\n",
    "    scores = np.random.rand(n,1)\n",
    "    best_m = int(1/alpha)\n",
    "    best_gamma = 1\n",
    "    best_q = 1\n",
    "    for m in candidates_m:\n",
    "        gamma, q = get_optimal_gamma(scores,n,alpha,m,epsilon)\n",
    "        if q < best_q:\n",
    "            best_q = q\n",
    "            best_m = m\n",
    "            best_gamma = gamma\n",
    "    return best_m, best_gamma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_conformal_scores(scores, labels):\n",
    "    conformal_scores = torch.tensor([scores[i,labels[i]] for i in range(scores.shape[0])]) \n",
    "    return conformal_scores \n",
    "\n",
    "def get_shat_from_scores_private(scores, alpha, epsilon, gamma, score_bins):\n",
    "    shat = get_private_quantile(scores, alpha, epsilon, gamma, score_bins)\n",
    "    return shat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce513cb-dee1-4ca1-815a-5662a8b381b9",
   "metadata": {},
   "source": [
    "### Helping function for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28509e61-3652-439e-929f-2155a1a09c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(modelname):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model by name.\n",
    "\n",
    "    Args:\n",
    "        modelname (str): Name of the model (e.g., 'ResNet152').\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Pre-trained model.\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Use the new weights parameter\n",
    "    if modelname == 'ResNet18':\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet50':\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet101':\n",
    "        model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet152':\n",
    "        model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNeXt101':\n",
    "        model = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'VGG16':\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ShuffleNet':\n",
    "        model = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'Inception':\n",
    "        model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'DenseNet161':\n",
    "        model = models.densenet161(weights=models.DenseNet161_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model {modelname} not supported\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Computes logits and targets from a model and loader\n",
    "def get_logits_targets(model, loader):\n",
    "    \"\"\"\n",
    "    Compute logits and targets for a dataset using a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pre-trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tuple of (logits, targets).\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logits = torch.zeros((len(loader.dataset), 1000))  # 1000 classes in ImageNet\n",
    "    labels = torch.zeros((len(loader.dataset),))\n",
    "    i = 0\n",
    "\n",
    "    print(f'Computing logits for model (only happens once).')\n",
    "    with torch.no_grad():\n",
    "        for x, targets in tqdm(loader, desc=\"Processing batches\"):\n",
    "            batch_logits = model(x.to(device)).detach().cpu()\n",
    "            logits[i:(i + x.shape[0]), :] = batch_logits\n",
    "            labels[i:(i + x.shape[0])] = targets.cpu()\n",
    "            i += x.shape[0]\n",
    "\n",
    "    # Construct the dataset\n",
    "    dataset_logits = torch.utils.data.TensorDataset(logits, labels.long())\n",
    "    return dataset_logits\n",
    "\n",
    "\n",
    "\n",
    "def get_logits_dataset(modelname, datasetname, datasetpath, cache= dirname + '/.cache/'):\n",
    "    fname = cache + datasetname + '/' + modelname + '.pkl' \n",
    "\n",
    "    # If the file exists, load and return it.\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as handle:\n",
    "            return pkl.load(handle)\n",
    "\n",
    "    # Else we will load our model, run it on the dataset, and save/return the output.\n",
    "    model = get_model(modelname)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std =[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "    \n",
    "    dataset = torchvision.datasets.ImageNet(datasetpath, split='val', transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "    # Get the logits and targets\n",
    "    dataset_logits = get_logits_targets(model, loader)\n",
    "\n",
    "    # Save the dataset \n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pkl.dump(dataset_logits, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dataset_logits\n",
    "\n",
    "    \n",
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_imagenet_classes():\n",
    "    df = pd.read_csv(dirname + '/map_clsloc.txt', delimiter=' ')\n",
    "    arr = df['name'].to_numpy()\n",
    "    return arr\n",
    "\n",
    "def get_metrics_precomputed(est_labels,labels,losses,num_classes):\n",
    "    labels = torch.nn.functional.one_hot(labels,num_classes)\n",
    "    empirical_losses = (losses.view(1,-1) * (labels * (1-est_labels))).sum(dim=1)\n",
    "    sizes = est_labels.sum(dim=1)\n",
    "    return empirical_losses, sizes \n",
    "\n",
    "\n",
    "def platt_logits(calib_dataset, max_iters=10, lr=0.01, epsilon=0.01):\n",
    "    calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1024, shuffle=False, pin_memory=True) \n",
    "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    T = nn.Parameter(torch.Tensor([1.3]).cuda())\n",
    "\n",
    "    optimizer = optim.SGD([T], lr=lr)\n",
    "    for iter in range(max_iters):\n",
    "        T_old = T.item()\n",
    "        for x, targets in calib_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            x.requires_grad = True\n",
    "            out = x/T\n",
    "            loss = nll_criterion(out, targets.long().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if abs(T_old - T.item()) < epsilon:\n",
    "            break\n",
    "    return T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd52f3-1cd7-48c8-99e5-db00988df143",
   "metadata": {},
   "source": [
    "### Helping function for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188ea1f-e744-4f65-8ba9-100e35bb7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(\n",
    "    conformal_scores,\n",
    "    raw_scores,\n",
    "    alpha,\n",
    "    epsilon,\n",
    "    num_calib,\n",
    "    seed,\n",
    "    score_bins,\n",
    "    gamma\n",
    "):\n",
    "    # Shuffle\n",
    "    total = conformal_scores.shape[0]\n",
    "    perm = torch.randperm(total)\n",
    "    conformal_scores = conformal_scores[perm]\n",
    "    raw_scores = raw_scores[perm]\n",
    "\n",
    "    # Split calibration and validation sets\n",
    "    calib_conformal_scores = 1 - conformal_scores[:num_calib]\n",
    "    val_conformal_scores   = 1 - conformal_scores[num_calib:]\n",
    "    calib_raw_scores       = 1 - raw_scores[:num_calib]\n",
    "    val_raw_scores         = 1 - raw_scores[num_calib:]\n",
    "\n",
    "    # ---- Private conformal thresholds ----\n",
    "    shat = get_shat_from_scores_private(\n",
    "        calib_conformal_scores, alpha, epsilon, gamma, score_bins\n",
    "    )\n",
    "\n",
    "    epsilon_conform = (epsilon**2)/2\n",
    "    threshold_PrivQuant = PrivQuant(\n",
    "        calib_conformal_scores, alpha, epsilon_conform, seed,\n",
    "        lower_bound=0, upper_bound=1, delta=1e-16\n",
    "    )\n",
    "\n",
    "    q = 1 - alpha\n",
    "    threshold_Lap_hist = dp_quantile_noisy_hist(\n",
    "        calib_conformal_scores, q, epsilon, seed\n",
    "    )\n",
    "\n",
    "    # ---- Coverage (correctness) ----\n",
    "    corrects            = (val_conformal_scores < shat)\n",
    "    corrects_PrivQuant  = (val_conformal_scores < threshold_PrivQuant)\n",
    "    corrects_Lap_hist   = (val_conformal_scores < threshold_Lap_hist)\n",
    "\n",
    "    # ---- Set sizes ----\n",
    "    sizes           = (val_raw_scores < shat).sum(dim=1)\n",
    "    sizes_PrivQuant = (val_raw_scores < threshold_PrivQuant).sum(dim=1)\n",
    "    sizes_Lap_hist  = (val_raw_scores < threshold_Lap_hist).sum(dim=1)\n",
    "\n",
    "    return (\n",
    "        corrects.float().mean().item(),\n",
    "        corrects_PrivQuant.float().mean().item(),\n",
    "        corrects_Lap_hist.float().mean().item(),\n",
    "        sizes,\n",
    "        sizes_PrivQuant,\n",
    "        sizes_Lap_hist,\n",
    "        shat,\n",
    "        threshold_PrivQuant,\n",
    "        threshold_Lap_hist\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370853b-09cb-432f-9664-0f0a546107fb",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7464a14-7e0f-466a-be58-09cd342795b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha, epsilons, num_calib, seed, num_trials, imagenet_val_dir):\n",
    "    # Clear old cache files\n",
    "    cache_files = glob.glob(f'.cache/opt_{alpha}_*_dataframe_trial.pkl')\n",
    "    for cache_file in cache_files:\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "\n",
    "        # Prepare cache directory\n",
    "        os.makedirs('.cache', exist_ok=True)\n",
    "        fname = f'.cache/opt_{alpha}_{epsilon}_{num_calib}_dataframe_trial.pkl'\n",
    "\n",
    "        # Compute optimal bins + gamma\n",
    "        mstar, gammastar = get_optimal_gamma_m(num_calib, alpha, epsilon)\n",
    "        m = mstar\n",
    "        gamma = gammastar\n",
    "        score_bins = np.linspace(0, 1, m)\n",
    "\n",
    "        # Expected column order\n",
    "        expected_columns = [\n",
    "            \"$\\\\hat{s}$\",\n",
    "            \"$\\\\hat{q}_$PCOQS\",\n",
    "            \"thres_Lap_hist\",\n",
    "            \"EXPONQ\",\n",
    "            \"PCOQS\",\n",
    "            \"Lap_hist\",\n",
    "            \"sizes_EXPONQ\",\n",
    "            \"sizes_PCOQS\",\n",
    "            \"sizes_Lap_hist\",\n",
    "            \"$\\\\alpha$\",\n",
    "            \"$\\\\epsilon$\"\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Try loading cache\n",
    "            df = pd.read_pickle(fname)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            \n",
    "            # Compute dataset + trials\n",
    "            dataset_precomputed = get_logits_dataset('ResNet152', 'Imagenet', imagenet_val_dir)\n",
    "            print('Dataset loaded')\n",
    "\n",
    "            classes_array = get_imagenet_classes()\n",
    "            T = platt_logits(dataset_precomputed)\n",
    "\n",
    "            logits, labels = dataset_precomputed.tensors\n",
    "            scores = (logits / T.cpu()).softmax(dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                conformal_scores = get_conformal_scores(scores, labels)\n",
    "\n",
    "                local_df_list = []\n",
    "                for i in tqdm(range(num_trials)):\n",
    "\n",
    "                    trial_seed = seed + i\n",
    "\n",
    "                    cvg1, cvg2, cvg3, szs1, szs2, szs3, shat, threshold_PrivQuant, threshold_Lap_hist = \\\n",
    "                        trial_precomputed(\n",
    "                            conformal_scores, scores, alpha, epsilon,\n",
    "                            num_calib, trial_seed, score_bins, gamma\n",
    "                        )\n",
    "\n",
    "                    # ---- FIXED SIZE FORMAT HERE ----\n",
    "                    dict_local = {\n",
    "                        \"$\\\\hat{s}$\": shat,\n",
    "                        \"$\\\\hat{q}_$PCOQS\": threshold_PrivQuant,\n",
    "                        \"thres_Lap_hist\": threshold_Lap_hist,\n",
    "\n",
    "                        \"EXPONQ\": cvg1,\n",
    "                        \"PCOQS\": cvg2,\n",
    "                        \"Lap_hist\": cvg3,\n",
    "\n",
    "                        # *** FIX: convert tensors -> list, then wrap in another list ***\n",
    "                        \"sizes_EXPONQ\": [szs1.cpu().tolist()],\n",
    "                        \"sizes_PCOQS\": [szs2.cpu().tolist()],\n",
    "                        \"sizes_Lap_hist\": [szs3.cpu().tolist()],\n",
    "\n",
    "                        \"$\\\\alpha$\": alpha,\n",
    "                        \"$\\\\epsilon$\": epsilon\n",
    "                    }\n",
    "\n",
    "                    df_local = pd.DataFrame(dict_local)\n",
    "                    local_df_list.append(df_local)\n",
    "\n",
    "                df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "\n",
    "                # Enforce column order\n",
    "                df = df.reindex(columns=expected_columns)\n",
    "\n",
    "                # Save cache\n",
    "                df.to_pickle(fname)\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set(palette='pastel', font='serif')\n",
    "    sns.set_style('white')\n",
    "    fix_randomness(seed=0)\n",
    "\n",
    "    imagenet_val_dir = '' # TODO: put your imagenet data directory here\n",
    "\n",
    "    alpha = 0.1\n",
    "    epsilons = [0.05, 0.1, 0.5, 1, 3, 5]\n",
    "    num_calib = 30000\n",
    "    num_trials = 100\n",
    "    seed = 123\n",
    "\n",
    "    results = experiment(\n",
    "        alpha, epsilons, num_calib,\n",
    "        seed, num_trials,\n",
    "        imagenet_val_dir=imagenet_val_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5ed98-8a13-49f6-a2e5-512a9e0a9122",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95fd76-cc27-4d95-a6a5-a306add5436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'df_list_ImageNet_results_trial.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "            pkl.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9119653-51a4-4443-bc91-7672bf543d3f",
   "metadata": {},
   "source": [
    "### Processing and saving the results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2147387-0d70-459e-9093-e9e692846a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_size_column(col):\n",
    "    \"\"\"\n",
    "    Extract size arrays when each cell is:\n",
    "        [1, 3, 3, 5, ...]   (flat list)\n",
    "    or occasionally wrapped like:\n",
    "        [[1, 3, 3, 5, ...]]\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    for cell in col:\n",
    "\n",
    "        # Skip invalid entries\n",
    "        if cell is None or (isinstance(cell, float) and np.isnan(cell)):\n",
    "            continue\n",
    "\n",
    "        inner = cell\n",
    "\n",
    "        # If nested one extra level, unwrap\n",
    "        if isinstance(inner, list) and len(inner) == 1 and isinstance(inner[0], list):\n",
    "            inner = inner[0]\n",
    "\n",
    "        # Now inner MUST be flat list\n",
    "        if not isinstance(inner, (list, np.ndarray)):\n",
    "            print(\"Warning: invalid size cell:\", inner)\n",
    "            continue\n",
    "\n",
    "        flattened.append(np.array(inner, dtype=float))\n",
    "\n",
    "    return flattened\n",
    "\n",
    "\n",
    "\n",
    "def compute_trial_averages(size_series):\n",
    "    \"\"\"\n",
    "    Compute mean set size for each trial.\n",
    "    Input: size_series where each row is: [ [size1, size2, ..., sizeN] ]\n",
    "    Output: array of size num_trials containing avg set size per trial.\n",
    "    \"\"\"\n",
    "    flattened = flatten_size_column(size_series)\n",
    "\n",
    "    if len(flattened) == 0:\n",
    "        print(\"Warning: No valid size data\")\n",
    "        return np.array([])\n",
    "\n",
    "    # All trials should have arrays of equal length\n",
    "    lengths = set(arr.size for arr in flattened)\n",
    "    if len(lengths) != 1:\n",
    "        print(\"Warning: Inconsistent evaluation lengths across trials:\", lengths)\n",
    "\n",
    "    # Compute average size per trial\n",
    "    return np.array([arr.mean() for arr in flattened])\n",
    "\n",
    "\n",
    "def safe_to_dataframe(data_dict):\n",
    "    \"\"\"Convert dictionary to DataFrame even if lists have different lengths.\"\"\"\n",
    "    if not data_dict:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    max_len = max(len(v) for v in data_dict.values())\n",
    "    padded = {\n",
    "        k: np.pad(v, (0, max_len - len(v)),\n",
    "                  mode=\"constant\", constant_values=np.nan)\n",
    "        for k, v in data_dict.items()\n",
    "    }\n",
    "    return pd.DataFrame(padded)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load your experiment() output\n",
    "    try:\n",
    "        with open('df_list_ImageNet_results_trial.pkl', 'rb') as f:\n",
    "            df_list = pkl.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found\")\n",
    "        return\n",
    "    except pkl.PickleError:\n",
    "        print(\"Error: Could not unpickle the file\")\n",
    "        return\n",
    "\n",
    "    epsilons = [0.05, 0.1, 0.5, 1, 3, 5]\n",
    "\n",
    "    results = {\n",
    "        'coverage': {eps: {} for eps in epsilons},\n",
    "        'avg_size': {eps: {} for eps in epsilons}\n",
    "    }\n",
    "\n",
    "    coverage_methods = [\"EXPONQ\", \"PCOQS\", \"Lap_hist\"]\n",
    "    size_methods = [\"EXPONQ\", \"PCOQS\", \"Lap_hist\"]\n",
    "\n",
    "    # ---- Parse each DataFrame ----\n",
    "    for eps, df in zip(epsilons, df_list):\n",
    "        try:\n",
    "            # ---- COVERAGE ----\n",
    "            for method in coverage_methods:\n",
    "                if method in df.columns:\n",
    "                    cov_data = df[method].dropna().astype(float).values\n",
    "                    results['coverage'][eps][method] = cov_data\n",
    "\n",
    "            # ---- SIZE ----\n",
    "            for method in size_methods:\n",
    "                size_key = f\"sizes_{method}\"\n",
    "                if size_key in df.columns:\n",
    "                    trial_avg_sizes = compute_trial_averages(df[size_key])\n",
    "\n",
    "                    if len(trial_avg_sizes) > 0:\n",
    "                        results['avg_size'][eps][method] = trial_avg_sizes\n",
    "                    else:\n",
    "                        print(f\"Warning: No size averages for {method} at eps={eps}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing epsilon={eps}: {str(e)}\")\n",
    "\n",
    "    # ---- SAVE COVERAGE ----\n",
    "    for eps in epsilons:\n",
    "        cov_dict = results['coverage'][eps]\n",
    "        if cov_dict:\n",
    "            df_cov = safe_to_dataframe(cov_dict)\n",
    "            if not df_cov.empty:\n",
    "                df_cov.to_csv(f'coverage_epsilon_{eps}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No coverage data for epsilon={eps}\")\n",
    "\n",
    "    # ---- SAVE AVERAGE SIZE ----\n",
    "    for eps in epsilons:\n",
    "        size_dict = results['avg_size'][eps]\n",
    "        if size_dict:\n",
    "            df_size = safe_to_dataframe(size_dict)\n",
    "            if not df_size.empty:\n",
    "                df_size.to_csv(f'avg_size_epsilon_{eps}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No avg size data for epsilon={eps}\")\n",
    "\n",
    "    print(\"Processing complete. Files saved:\")\n",
    "    print(\"  coverage_epsilon_[eps].csv\")\n",
    "    print(\"  avg_size_epsilon_[eps].csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
