{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f21417a-2480-4912-bf62-f1056d09f625",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd5103-91cb-44c3-96d1-656de08253de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pathlib\n",
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import shutil\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965eba8-efe5-41e6-b77c-b63d671e5d35",
   "metadata": {},
   "source": [
    "### NoisyRC function and PrivQuant algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af0dac-7e14-435c-97d4-bd42bbb14b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRC(range_bounds, D, sigma):\n",
    "    \"\"\"\n",
    "    Noisy Range Count for float values with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    range_bounds (tuple): A tuple (a, b) representing the range [a, b].\n",
    "    D (list): The sorted dataset.\n",
    "    sigma (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    int: The noisy count of elements in the range [a, b].\n",
    "    \"\"\"\n",
    "    a, b = range_bounds\n",
    "    count = sum(1 for z in D if a <= z <= b)\n",
    "    noise = np.random.normal(0, sigma)\n",
    "    noisy_count = count + noise\n",
    "    return max(0, int(np.floor(noisy_count)))  # Ensure non-negative count\n",
    "\n",
    "def PrivQuant(D, alpha, rho, seed, lower_bound=0, upper_bound=1, delta=1e-10):\n",
    "    \"\"\"\n",
    "    Differentially Private Quantile Approximation Algorithm without integer conversion.\n",
    "\n",
    "    Parameters:\n",
    "    D (list): The sorted dataset.\n",
    "    alpha (float): The quantile level (e.g., 0.5 for median).\n",
    "    rho (float): The privacy parameter (smaller = more private).\n",
    "    lower_bound (float): Lower bound of the search space.\n",
    "    upper_bound (float): Upper bound of the search space.\n",
    "    delta (float): Small positive value to ensure convergence.\n",
    "\n",
    "    Returns:\n",
    "    float: A differentially private approximation of the quantile x_{(m)}.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n = len(D)\n",
    "    max_iterations = int(np.ceil(np.log2((upper_bound - lower_bound) / delta)))\n",
    "    sigma = np.sqrt(max_iterations / (2 * rho)) # Noise scale for Gaussian mechanism\n",
    "    m = int(np.ceil((1 - alpha) * (n + 1)))\n",
    "\n",
    "    left, right = lower_bound, upper_bound\n",
    "    random.seed(seed)\n",
    "    for i in range(max_iterations):\n",
    "        mid = (left + right) / 2\n",
    "        c = NoisyRC((lower_bound, mid), D, sigma)\n",
    "        \n",
    "        if c < m:\n",
    "            left = mid + delta\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return np.round((left + right) / 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b6dbb-f995-4a26-860a-87160b934473",
   "metadata": {},
   "source": [
    "### Helping Function for  Lap-Hist Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a78c5f-353c-42f1-98a9-81be27707860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_quantile_noisy_hist(x, q, epsilon, seed, bins=50, domain=(0.0, 1.0), rng=None):\n",
    "    \"\"\"\n",
    "    Differentially private quantile using a Laplace-noised histogram (ε-DP).\n",
    "\n",
    "    Args:\n",
    "        x (array-like): data vector (numeric).\n",
    "        q (float): desired quantile in (0,1).\n",
    "        epsilon (float): privacy budget for the entire histogram.\n",
    "        domain (tuple): (lo, hi) public bounds for clipping/binning.\n",
    "        bins (int): number of fixed, public bins.\n",
    "        rng: np.random.Generator (optional).\n",
    "\n",
    "    Returns:\n",
    "        float: DP quantile estimate (can lie between data points).\n",
    "\n",
    "    Privacy & assumptions:\n",
    "        - Data are clipped to the public domain (lo, hi).\n",
    "        - Build a fixed-bin histogram, add Lap(1/ε) noise to each bin count.\n",
    "        - Because each record contributes to exactly one bin, releasing\n",
    "          the full noisy histogram is ε-DP under add/remove adjacency.\n",
    "        - Quantile is computed from the noisy cumulative counts.\n",
    "\n",
    "    Notes:\n",
    "        - Works best if a reasonable public domain is known.\n",
    "        - For stability, negative noisy counts are floored at 0.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty.\")\n",
    "    if not (0 < q < 1):\n",
    "        raise ValueError(\"q must be in (0,1).\")\n",
    "    if epsilon <= 0:\n",
    "        raise ValueError(\"epsilon must be > 0.\")\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    lo, hi = domain\n",
    "    if not (lo < hi):\n",
    "        raise ValueError(\"domain must satisfy lo < hi.\")\n",
    "\n",
    "    # Clip to public domain\n",
    "    xc = np.clip(x, lo, hi)\n",
    "\n",
    "    # Fixed public bins\n",
    "    edges = np.linspace(lo, hi, bins + 1)\n",
    "    #print(f\"Bins: {edges}\")\n",
    "    counts, _ = np.histogram(xc, bins=edges)\n",
    "    #print(f\"Counts of histogram: {counts}\")\n",
    "\n",
    "    # Laplace noise to each bin (scale = 1/ε)\n",
    "    noise = rng.laplace(loc=0.0, scale=1.0/epsilon, size=bins)\n",
    "    #print(f\"Noise for each bin: {noise}\")\n",
    "    noisy = np.maximum(counts + noise, 0.0)\n",
    "    #print(noisy)\n",
    "\n",
    "    # Cumulative proportion\n",
    "    csum = np.cumsum(noisy)\n",
    "    if csum[-1] <= 0:\n",
    "        # extremely unlikely unless ε is tiny and n is tiny\n",
    "        return float(np.median(xc))\n",
    "\n",
    "    target = q * csum[-1]\n",
    "    j = np.searchsorted(csum, target)  # first bin reaching the target\n",
    "\n",
    "    j = int(np.clip(j, 0, bins - 1))\n",
    "    # Linear interpolation within the bin (simple, uniform-within-bin)\n",
    "    bin_lo, bin_hi = edges[j], edges[j + 1]\n",
    "    prev = csum[j - 1] if j > 0 else 0.0\n",
    "    within = (target - prev) / max(noisy[j], 1e-12)\n",
    "    within = np.clip(within, 0.0, 1.0)\n",
    "    return float(bin_lo + within * (bin_hi - bin_lo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9767651-fb13-488b-b078-f2ae25ad061a",
   "metadata": {},
   "source": [
    "### Helping Function for EXPONQ and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770b9ec-1644-490d-b98d-beb03fc08619",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = str(pathlib.Path().absolute())\n",
    "\n",
    "\n",
    "def get_qtilde(n,alpha,gamma,epsilon,m):\n",
    "    qtilde = (n+1)*(1-alpha)/(n*(1-gamma*alpha))+2/(epsilon*n)*np.log(m/(gamma*alpha))\n",
    "    qtilde = min(qtilde, 1-1e-12)\n",
    "    return qtilde\n",
    "\n",
    "def generate_scores(n):\n",
    "    return np.random.uniform(size=(n,))\n",
    "\n",
    "def hist_2_cdf(cumsum, bins, n):\n",
    "    def _cdf(t):\n",
    "        if t > bins[-2]:\n",
    "            return 1.0\n",
    "        elif t < bins[1]:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1-cumsum[np.searchsorted(bins, t)]/n\n",
    "    return _cdf\n",
    "\n",
    "def get_private_quantile(scores, alpha, epsilon, gamma, bins):\n",
    "    n = scores.shape[0]\n",
    "    epsilon_normed = epsilon*min(alpha, 1-alpha)\n",
    "    # Get the quantile\n",
    "    qtilde = get_qtilde(n, alpha, gamma, epsilon, bins.shape[0])\n",
    "    scores = scores.squeeze()\n",
    "    score_to_bin = np.digitize(scores,bins)\n",
    "    binned_scores = bins[np.minimum(score_to_bin,bins.shape[0]-1)]\n",
    "    w1 = np.digitize(binned_scores, bins)\n",
    "    w2 = np.digitize(binned_scores, bins, right=True)\n",
    "    # Clip bins\n",
    "    w1 = np.maximum(np.minimum(w1,bins.shape[0]-1),0)\n",
    "    w2 = np.maximum(np.minimum(w2,bins.shape[0]-1),0)\n",
    "    lower_mass = np.bincount(w1,minlength=bins.shape[0]).cumsum()/qtilde\n",
    "    upper_mass = (n-np.bincount(w2,minlength=bins.shape[0]).cumsum())/(1-qtilde)\n",
    "    w = np.maximum( lower_mass , upper_mass )\n",
    "    sampling_probabilities = softmax(-(epsilon_normed/2)*w)\n",
    "    # Check\n",
    "    sampling_probabilities = sampling_probabilities/sampling_probabilities.sum()\n",
    "    qhat = np.random.choice(bins,p=sampling_probabilities)\n",
    "    return qhat\n",
    "\n",
    "# Optimal gamma is a root.\n",
    "def get_optimal_gamma(scores,n,alpha,m,epsilon):\n",
    "    a = alpha**2\n",
    "    b = - ( alpha*epsilon*(n+1)*(1-alpha)/2 + 2*alpha )\n",
    "    c = 1\n",
    "    best_q = 1\n",
    "    gamma1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "    gamma2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "\n",
    "    gamma1 = min(max(gamma1,1e-12),1-1e-12)\n",
    "    gamma2 = min(max(gamma2,1e-12),1-1e-12)\n",
    "\n",
    "    bins = np.linspace(0,1,m)\n",
    "\n",
    "    q1 = get_private_quantile(scores, alpha, epsilon, gamma1, bins)\n",
    "    q2 = get_private_quantile(scores, alpha, epsilon, gamma2, bins)\n",
    "\n",
    "    return (gamma1, q1) if q1 < q2 else (gamma2, q2)\n",
    "\n",
    "def get_optimal_gamma_m(n, alpha, epsilon):\n",
    "    candidates_m = np.logspace(4,6,50).astype(int)\n",
    "    scores = np.random.rand(n,1)\n",
    "    best_m = int(1/alpha)\n",
    "    best_gamma = 1\n",
    "    best_q = 1\n",
    "    for m in candidates_m:\n",
    "        gamma, q = get_optimal_gamma(scores,n,alpha,m,epsilon)\n",
    "        if q < best_q:\n",
    "            best_q = q\n",
    "            best_m = m\n",
    "            best_gamma = gamma\n",
    "    return best_m, best_gamma\n",
    "\n",
    "\n",
    "\n",
    "def get_conformal_scores(scores, labels):\n",
    "    conformal_scores = torch.tensor([scores[i,labels[i]] for i in range(scores.shape[0])]) \n",
    "    return conformal_scores \n",
    "\n",
    "def get_shat_from_scores_private(scores, alpha, epsilon, gamma, score_bins):\n",
    "    shat = get_private_quantile(scores, alpha, epsilon, gamma, score_bins)\n",
    "    return shat \n",
    "\n",
    "\n",
    "def get_shat_from_scores(scores, alpha):\n",
    "    return np.quantile(scores,1-alpha)\n",
    "\n",
    "# def get_model(private=False, feature_extract=True, cache= dirname + '/.cache/'):\n",
    "#     model_ft = models.resnet18(pretrained=True)\n",
    "#     set_parameter_requires_grad(model_ft, feature_extract)\n",
    "#     num_ftrs = model_ft.fc.in_features\n",
    "#     model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "#     data = torch.load('./.cache/nonprivatemodel_best.pth.tar')\n",
    "\n",
    "#     model_ft.load_state_dict(data)\n",
    "#     model_ft.cuda()\n",
    "#     model_ft.eval()\n",
    "\n",
    "#     return model_ft\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(private, feature_extract=True, cache= dirname + '/.cache/'):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "    if private:\n",
    "        model_path = \"\"  #TO DO: Put path to private model here\n",
    "    else:\n",
    "        model_path = \"\"  #TO DO: Put path to non-private model here\n",
    "\n",
    "    # Load the model state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Remove the \"_module.\" prefix from keys if present\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_module.\"):\n",
    "            name = k[8:]  # Remove \"_module.\" prefix\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the modified state dict\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computes logits and targets from a model and loader\n",
    "def get_logits_targets(model, loader):\n",
    "    logits = torch.zeros((len(loader.dataset), 3)) # 3 classes in XRAY.\n",
    "    labels = torch.zeros((len(loader.dataset),))\n",
    "    i = 0\n",
    "    print(f'Computing logits for model (only happens once).')\n",
    "    with torch.no_grad():\n",
    "        for x, targets in tqdm(loader):\n",
    "            batch_logits = model(x.cuda()).detach().cpu()\n",
    "            logits[i:(i+x.shape[0]), :] = batch_logits\n",
    "            labels[i:(i+x.shape[0])] = targets.cpu()\n",
    "            i = i + x.shape[0]\n",
    "    \n",
    "    # Construct the dataset\n",
    "    dataset_logits = torch.utils.data.TensorDataset(logits, labels.long()) \n",
    "    return dataset_logits\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed):\n",
    "    # Create training and validation datasets\n",
    "    input_size = 224\n",
    "    batch_size = 256\n",
    "\n",
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    fix_randomness(seed)\n",
    "    image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(datasetpath, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "    temp = torch.utils.data.ConcatDataset([image_datasets['train'],image_datasets['val']])\n",
    "    image_datasets['train'], image_datasets['val'] = torch.utils.data.random_split(temp,[len(temp)-num_calib-num_val,num_calib+num_val])\n",
    "    return image_datasets\n",
    "    \n",
    "\n",
    "def get_logits_dataset(private, datasetname, datasetpath, num_calib, num_val, seed, cache= dirname + '/.cache/'):\n",
    "    fname = cache + datasetname + '/' + 'private' + '.pkl'  if private else cache + datasetname + '/nonprivate.pkl'\n",
    "    batch_size = 256\n",
    "\n",
    "    image_datasets = get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed)\n",
    "    # If the file exists, load and return it.\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as handle:\n",
    "            return pickle.load(handle), image_datasets\n",
    "\n",
    "    # Else we will load our model, run it on the dataset, and save/return the output.\n",
    "    model = get_model(private, True)\n",
    "\n",
    "    # get the datasets and loaders\n",
    "    image_datasets = get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed)\n",
    "\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "    # Get the logits and targets\n",
    "    dataset_logits_dict = {x: get_logits_targets(model, dataloaders_dict[x]) for x in ['train','val']}\n",
    "\n",
    "    # Save the dataset \n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(dataset_logits_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dataset_logits_dict, image_datasets\n",
    "\n",
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_metrics_precomputed(est_labels,labels,losses,num_classes):\n",
    "    labels = torch.nn.functional.one_hot(labels,num_classes)\n",
    "    empirical_losses = (losses.view(1,-1) * (labels * (1-est_labels))).sum(dim=1)\n",
    "    sizes = est_labels.sum(dim=1)\n",
    "    return empirical_losses, sizes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b6cdae-07e1-43b1-8d5d-ae7aa33d9b48",
   "metadata": {},
   "source": [
    "### Helping function for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f35910-6933-432d-a89c-7651c9bb2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=\"checkpoint.tar\", private=False):\n",
    "    root = f'./.cache/'\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    root = root + 'private' if private else root + 'nonprivate'\n",
    "    torch.save(state, root+filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(root+filename, root+\"model_best.pth.tar\")\n",
    "\n",
    "def fix_randomness(seed):\n",
    "    ### Fix randomness \n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def platt_logits(calib_dataset, max_iters=10, lr=0.01, epsilon=0.01):\n",
    "    calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1024, shuffle=False, pin_memory=True) \n",
    "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    T = nn.Parameter(torch.Tensor([1.3]).cuda())\n",
    "\n",
    "    optimizer = optim.SGD([T], lr=lr)\n",
    "    for iter in range(max_iters):\n",
    "        T_old = T.item()\n",
    "        for x, targets in calib_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            x.requires_grad = True\n",
    "            out = x/T\n",
    "            loss = nll_criterion(out, targets.long().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if abs(T_old - T.item()) < epsilon:\n",
    "            break\n",
    "    return T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55db9d-952d-49a2-b7e3-a9e9d48ca70c",
   "metadata": {},
   "source": [
    "### Helping function for comformal prediction experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb466-5196-4332-a540-e0629d63baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(conformal_scores, raw_scores, alpha, epsilon, gamma, score_bins, num_calib, seed, privateconformal):\n",
    "    total = conformal_scores.shape[0]\n",
    "    perm = torch.randperm(conformal_scores.shape[0])\n",
    "    conformal_scores = conformal_scores[perm]\n",
    "    raw_scores = raw_scores[perm]\n",
    "    calib_conformal_scores, val_conformal_scores = (1-conformal_scores[0:num_calib], 1-conformal_scores[num_calib:])\n",
    "    calib_raw_scores, val_raw_scores = (1-raw_scores[0:num_calib], 1-raw_scores[num_calib:])\n",
    "\n",
    "    # Always compute non-private results\n",
    "    threshold_nonpriv = get_shat_from_scores(calib_conformal_scores, alpha)\n",
    "    corrects_nonpriv = (val_conformal_scores < threshold_nonpriv)\n",
    "    sizes_nonpriv = (val_raw_scores < threshold_nonpriv).sum(dim=1)\n",
    "\n",
    "    # Initialize private outputs (so they exist even when privateconformal is False)\n",
    "    shat = None\n",
    "    threshold_PrivQuant = None\n",
    "    threshold_Lap_hist = None\n",
    "\n",
    "    corrects = None\n",
    "    corrects_PrivQuant = None\n",
    "    corrects_Lap_hist = None\n",
    "\n",
    "    sizes = None\n",
    "    sizes_PrivQuant = None\n",
    "    sizes_Lap_hist = None\n",
    "\n",
    "    if privateconformal:\n",
    "        # Only compute private results if privateconformal is True\n",
    "        shat = get_shat_from_scores_private(calib_conformal_scores, alpha, epsilon, gamma, score_bins)\n",
    "\n",
    "        # privacy budget for PrivQuant \n",
    "        epsilon_conform = (epsilon**2) / 2\n",
    "        threshold_PrivQuant = PrivQuant(calib_conformal_scores, alpha, epsilon_conform, seed)\n",
    "\n",
    "        q = 1 - alpha\n",
    "        threshold_Lap_hist = dp_quantile_noisy_hist(calib_conformal_scores, q, epsilon, seed)\n",
    "\n",
    "        corrects = (val_conformal_scores < shat)\n",
    "        corrects_PrivQuant = (val_conformal_scores < threshold_PrivQuant)\n",
    "        corrects_Lap_hist = (val_conformal_scores < threshold_Lap_hist)\n",
    "\n",
    "        sizes = (val_raw_scores < shat).sum(dim=1)\n",
    "        sizes_PrivQuant = (val_raw_scores < threshold_PrivQuant).sum(dim=1)\n",
    "        sizes_Lap_hist = (val_raw_scores < threshold_Lap_hist).sum(dim=1)\n",
    "\n",
    "    # Build the 12-tuple return (consistent ordering)\n",
    "    # Order: corrects, corrects_PrivQuant, corrects_Lap_hist, corrects_nonpriv,\n",
    "    #        sizes, sizes_PrivQuant, sizes_Lap_hist, sizes_nonpriv,\n",
    "    #        shat, threshold_PrivQuant, threshold_Lap_hist, threshold_nonpriv\n",
    "\n",
    "    return (\n",
    "        corrects.float().mean().item() if corrects is not None else np.nan,\n",
    "        corrects_PrivQuant.float().mean().item() if corrects_PrivQuant is not None else np.nan,\n",
    "        corrects_Lap_hist.float().mean().item() if corrects_Lap_hist is not None else np.nan,\n",
    "        corrects_nonpriv.float().mean().item(),\n",
    "        sizes if sizes is not None else torch.tensor([]),\n",
    "        sizes_PrivQuant if sizes_PrivQuant is not None else torch.tensor([]),\n",
    "        sizes_Lap_hist if sizes_Lap_hist is not None else torch.tensor([]),\n",
    "        sizes_nonpriv,\n",
    "        float(shat) if shat is not None else np.nan,\n",
    "        float(threshold_PrivQuant) if threshold_PrivQuant is not None else np.nan,\n",
    "        float(threshold_Lap_hist) if threshold_Lap_hist is not None else np.nan,\n",
    "        float(threshold_nonpriv)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3526b38-35a6-4f93-933e-2f9781339107",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fa6bb-e3d9-416d-89a9-12dbed507976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, is_private=False, privacy_engine=None):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Define paths for saving the best models\n",
    "    if is_private:\n",
    "        model_path = \".../best_model_private.pth\" #TO DO: Put the complete path where you want to save the model best_model_private.pth\n",
    "    else:\n",
    "        model_path = \".../best_model_nonprivate.pth\" #TO DO: Put the complete path where you want to save the model best_model_nonprivate.pth\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if is_private:\n",
    "                            # For private models, use the privacy engine's step\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()  # Opacus requires zero_grad after step\n",
    "                        else:\n",
    "                            # For non-private models, use standard step\n",
    "                            optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # Save the best model weights\n",
    "                torch.save(best_model_wts, model_path)\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"PyTorch Version: \", torch.__version__)\n",
    "    # print(\"Torchvision Version: \", torchvision.__version__)\n",
    "\n",
    "    # Top level data directory\n",
    "    data_dir = \".../covid_chest_xray/data/imagefolder\"  #TO DO: Put the directory where you saved the CoronaHack dataset\n",
    "    EPSILON =1\n",
    "    DELTA = 1e-5\n",
    "    MAX_GRAD_NORM = 2  # Maximum gradient norm for clipping\n",
    "    # Number of classes in the dataset\n",
    "    num_classes = 3\n",
    "\n",
    "    # Batch size for training\n",
    "    batch_size = 8\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 15\n",
    "\n",
    "    # Flag for feature extracting\n",
    "    feature_extract = True\n",
    "\n",
    "    # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Create training and validation dataloaders\n",
    "    image_datasets = get_dataset_shuffle_split(data_dir, num_calib=1000, num_val=500, seed=0)\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Send the model to GPU\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in model_ft.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name, param in model_ft.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    # Setup the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Non-private model training\n",
    "    print(\"Training non-private model...\")\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False, is_private=False)\n",
    "\n",
    "    # Private model training\n",
    "    print(\"Training private model...\")\n",
    "    model_ft_private = models.resnet18(pretrained=True)\n",
    "    set_parameter_requires_grad(model_ft_private, feature_extract)\n",
    "    model_ft_private.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    model_ft_private = model_ft_private.to(device)\n",
    "\n",
    "    optimizer_ft_private = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Add differential privacy using Opacus\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model_ft_private, optimizer_ft_private, dataloaders_dict['train'] = privacy_engine.make_private_with_epsilon(\n",
    "    module=model_ft_private,\n",
    "    optimizer=optim.SGD(model_ft_private.parameters(), lr=0.001, momentum=0.9),\n",
    "    data_loader=dataloaders_dict['train'],\n",
    "    epochs=num_epochs,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,)\n",
    "\n",
    "\n",
    "    model_ft_private, hist_private = train_model(model_ft_private, dataloaders_dict, criterion, optimizer_ft_private, num_epochs=num_epochs, is_inception=False, is_private=True, privacy_engine=privacy_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f4692-88dc-48e1-9b19-9658364658d0",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b07da-c13c-4cb8-8248-2611b9b6cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha, epsilon, num_calib, num_val, seed, datasetpath, privatemodel, privateconformal):\n",
    "    #df_list = []\n",
    "    mstar, gammastar = get_optimal_gamma_m(num_calib, alpha, epsilon)\n",
    "    score_bins = np.linspace(0, 1, mstar)\n",
    "    fname = f'.cache/opt_{alpha}_{epsilon}_{num_calib}_{mstar}bins_pm_{privatemodel}_pc_{privateconformal}_dataframe.pkl'\n",
    "    #fname = f'.cache/opt_{alpha}_{epsilon}_{num_calib}_{mstar}bins_dataframe.pkl'\n",
    "\n",
    "\n",
    "    # Define the expected columns\n",
    "    expected_columns = [\"$\\\\hat{s}$\", \"$\\\\hat{q}_$PrivQuant\", \"threshold_Lap_hist\", \"threshold_nonpriv\", \"Anas et. al\", \"PrivQuant\",\"Lap_hist\", \"NonprivQuant\", \"sizes_Anas et. al\", \"sizes_PrivQuant\", \"sizes_Lap_hist\", \"sizes_NonprivQuant\", \"$\\\\alpha$\", \"$\\\\epsilon$\"]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(fname)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    all_data, image_dataset = get_logits_dataset(privatemodel, 'xray', datasetpath, num_calib, num_val, seed=0, cache=dirname + '/.cache/')\n",
    "    print('Dataset loaded')\n",
    "    dataset_precomputed = all_data['val']\n",
    "\n",
    "    classes_array = ['bacterial pneumonia', 'normal', 'viral pneumonia']\n",
    "    T = platt_logits(dataset_precomputed)\n",
    "\n",
    "    logits, labels = dataset_precomputed.tensors\n",
    "    scores = (logits / T.cpu()).softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        conformal_scores = get_conformal_scores(scores, labels)\n",
    "        local_df_list = []\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            seed = seed +i\n",
    "            cvg1, cvg2, cvg3,cvg4, szs1, szs2, szs3,szs4, shat, threshold_PrivQuant,threshold_Lap_hist, threshold_nonpriv = trial_precomputed(conformal_scores, scores, alpha, epsilon, gammastar, score_bins, num_calib, seed, privateconformal)\n",
    "            dict_local = {\n",
    "                \"NonprivQuant\": cvg4,\n",
    "                \"sizes_NonprivQuant\": [szs4],\n",
    "                \"Anas et. al\": cvg1 if privateconformal else np.nan,\n",
    "                \"PrivQuant\": cvg2 if privateconformal else np.nan,\n",
    "                \"Lap_hist\": cvg3 if privateconformal else np.nan,\n",
    "                \"sizes_Anas et. al\": [szs1] if privateconformal and szs1 is not None else [torch.tensor([])],\n",
    "                \"sizes_PrivQuant\": [szs2] if privateconformal and szs2 is not None else [torch.tensor([])],\n",
    "                \"sizes_Lap_hist\": [szs3] if privateconformal and szs3 is not None else [torch.tensor([])],\n",
    "                \"$\\\\hat{s}$\": shat if privateconformal else np.nan,\n",
    "                \"$\\\\hat{q}_$PrivQuant\": threshold_PrivQuant if privateconformal else np.nan,\n",
    "                \"Lap_hist\": threshold_Lap_hist if privateconformal else np.nan,\n",
    "                \"threshold_nonpriv\": threshold_nonpriv,\n",
    "                \"$\\\\alpha$\": alpha,\n",
    "                \"$\\\\epsilon$\":epsilon,\n",
    "                \"PrivateConformal\": privateconformal,  \n",
    "                \"PrivateModel\": privatemodel          \n",
    "    }\n",
    "            df_local = pd.DataFrame(dict_local)\n",
    "            local_df_list.append(df_local)\n",
    "\n",
    "        # Combine all local DataFrames into one\n",
    "        df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "\n",
    "        os.makedirs('.cache', exist_ok=True)\n",
    "        df.to_pickle(fname)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shutil.rmtree('.cache', ignore_errors=True)\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set(palette='pastel', font='serif')\n",
    "    sns.set_style('white')\n",
    "    fix_randomness(seed=0)\n",
    "\n",
    "    datasetpath = '.../covid_chest_xray/data/imagefolder'  #TO DO: Put complete path to dataset here\n",
    "    privateconformals = [False, True]\n",
    "    privatemodels = [False, True]\n",
    "\n",
    "    alpha = 0.1\n",
    "    epsilon = 1\n",
    "    num_calib = 1000\n",
    "    num_val = 500\n",
    "    num_trials = 1000\n",
    "    seed = 123\n",
    "\n",
    "    save_path = 'df_list_Corona_Hack.pkl'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "    else:\n",
    "        df_list = []\n",
    "        for privateconformal in privateconformals:\n",
    "            for privatemodel in privatemodels:\n",
    "                df_list.append(\n",
    "                    experiment(alpha, epsilon, num_calib, num_val, seed,\n",
    "                               datasetpath=datasetpath,\n",
    "                               privatemodel=privatemodel,\n",
    "                               privateconformal=privateconformal)\n",
    "                )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d00bc-356d-41d9-a437-9b7eb00382af",
   "metadata": {},
   "source": [
    "### Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3276fd4-8a8a-4b87-879c-e698acdc4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the result of the experiment\n",
    "with open(save_path, 'wb') as f:\n",
    "            pickle.dump(df_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd808f74-906e-4c28-b552-54ee72fbc14c",
   "metadata": {},
   "source": [
    "### Processing the results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae569a1d-f4cb-4c8b-97cb-4055dd3ba5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trial_averages(size_series, trials=1000, eval_points=500):\n",
    "    \"\"\"\n",
    "    Compute average set size per trial from size series\n",
    "    Returns: Array of 1000 average sizes (one per trial)\n",
    "    \"\"\"\n",
    "    # First flatten and convert all values to floats\n",
    "    sizes = []\n",
    "    for val in size_series.explode().dropna():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            sizes.append(float(val.item()))\n",
    "        else:\n",
    "            sizes.append(float(val))\n",
    "    \n",
    "    if len(sizes) != trials * eval_points:\n",
    "        print(f\"Warning: Expected {trials*eval_points} size points, got {len(sizes)}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    # Reshape to (trials, eval_points) and compute trial averages\n",
    "    size_array = np.array(sizes).reshape(trials, eval_points)\n",
    "    return np.mean(size_array, axis=1)\n",
    "\n",
    "def safe_to_dataframe(data_dict):\n",
    "    \"\"\"Convert dictionary to DataFrame, handling unequal lengths\"\"\"\n",
    "    if not data_dict:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    max_len = max(len(v) for v in data_dict.values())\n",
    "    padded = {k: np.pad(v, (0, max_len - len(v)), \n",
    "             mode='constant', constant_values=np.nan)\n",
    "             for k, v in data_dict.items()}\n",
    "    return pd.DataFrame(padded)\n",
    "\n",
    "def main():\n",
    "    # Load your data\n",
    "    try:\n",
    "        with open('df_list_Corona_Hack.pkl', 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found\")\n",
    "        return\n",
    "    except pickle.PickleError:\n",
    "        print(\"Error: Could not unpickle the file\")\n",
    "        return\n",
    "\n",
    "    # Setting names mapping\n",
    "    setting_names = {\n",
    "        (False, False): \"NonPrivateModel_NonPrivateConformal\",\n",
    "        (False, True): \"NonPrivateModel_PrivateConformal\",\n",
    "        (True, False): \"PrivateModel_NonPrivateConformal\",\n",
    "        (True, True): \"PrivateModel_PrivateConformal\"\n",
    "    }\n",
    "\n",
    "    # Initialize storage\n",
    "    results = {\n",
    "        'coverage': {setting: {} for setting in setting_names.values()},\n",
    "        'avg_size': {setting: {} for setting in setting_names.values()}  # For trial averages\n",
    "    }\n",
    "\n",
    "    # Process each setting's DataFrame\n",
    "    for df_idx, df in enumerate(df_list):\n",
    "        try:\n",
    "            private_model = df[\"PrivateModel\"].iloc[0]\n",
    "            private_conformal = df[\"PrivateConformal\"].iloc[0]\n",
    "            setting = setting_names[(private_model, private_conformal)]\n",
    "            \n",
    "            # Coverage data (unchanged)\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\", \"Lap_hist\"]:\n",
    "                if method in df.columns:\n",
    "                    cov_data = df[method].dropna().astype(float).values\n",
    "                    results['coverage'][setting][method] = cov_data\n",
    "            \n",
    "            # Size data - compute trial averages\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\", \"Lap_hist\"]:\n",
    "                size_key = f\"sizes_{method}\"\n",
    "                if size_key in df.columns:\n",
    "                    trial_avgs = compute_trial_averages(df[size_key])\n",
    "                    if len(trial_avgs) > 0:\n",
    "                        results['avg_size'][setting][method] = trial_avgs\n",
    "                    else:\n",
    "                        print(f\"Warning: No size averages for {method} in {setting} (DF #{df_idx+1})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataframe #{df_idx+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save coverage data\n",
    "    for setting in setting_names.values():\n",
    "        if results['coverage'][setting]:\n",
    "            df = safe_to_dataframe(results['coverage'][setting])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'coverage_{setting}.csv', index=False)\n",
    "\n",
    "    # Save average size data (one file per setting)\n",
    "    for setting in setting_names.values():\n",
    "        if results['avg_size'][setting]:\n",
    "            df = safe_to_dataframe(results['avg_size'][setting])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'avg_size_{setting}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No average size data for {setting}\")\n",
    "\n",
    "    print(\"Processing complete. Files saved:\")\n",
    "    print(\"- coverage_[setting].csv\")\n",
    "    print(\"- avg_size_[setting].csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
