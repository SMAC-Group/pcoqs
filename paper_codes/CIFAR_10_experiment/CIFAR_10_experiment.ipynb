{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f381e84-3cbd-44c3-82b1-de7b62217664",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbbbe6-d2c7-452a-94d5-b745497af0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.accountants import create_accountant\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os, sys, inspect\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import binom\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import binom, beta\n",
    "from scipy.special import softmax\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b55e5a-2240-41c7-82ec-9a70ba27aba2",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7050969-0f8a-4efb-9994-c2f3f6c0155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(num_classes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.GroupNorm(8, 64),  # GroupNorm with 8 groups\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "        nn.GroupNorm(8, 128),  # GroupNorm with 8 groups\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "        nn.GroupNorm(8, 192),  # GroupNorm with 8 groups\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(192, 256, kernel_size=3, stride=1, padding=1),\n",
    "        nn.GroupNorm(8, 256),  # GroupNorm with 8 groups\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        nn.Linear(256, num_classes, bias=True),\n",
    "        #nn.Dropout(0.5),  # Add dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af6530-fea7-4b09-a617-0da18a32ddcd",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb800dac-1e5a-4e73-b16b-7d9235d86bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Add color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=252, shuffle=True)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=252, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d87d822-f566-4742-b409-8a483edc89c4",
   "metadata": {},
   "source": [
    "### Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554cbfa-0b5b-495d-8987-b918d4c23a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, privacy_engine=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2377df-727c-4ef8-acca-63b7b21f51ba",
   "metadata": {},
   "source": [
    "### Training and saving the private model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae460edc-7ea9-462d-b833-5d593272db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Expand the range of alphas for tighter privacy bounds\n",
    "create_accountant(\"rdp\").alphas = list(range(2, 100)) + [128, 256, 512, 1024]\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 90  # Train epochs\n",
    "EPSILON = 8  # model privacy budget\n",
    "DELTA = 1e-5\n",
    "MAX_GRAD_NORM = 2  # Maximum gradient norm for clipping\n",
    "INITIAL_LR = 0.01  # Initial learning rate\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = convnet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=INITIAL_LR, weight_decay=1e-4)  # Using RMSprop with weight decay\n",
    "\n",
    "# Initialize PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Calculate learning rate using cosine schedule\n",
    "    lr = INITIAL_LR * 0.5 * (1 + np.cos(np.pi * epoch / (EPOCHS + 1)))\n",
    "    \n",
    "    # Update the optimizer's learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, privacy_engine)\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "    # Compute privacy budget (epsilon) for the current epoch\n",
    "    epsilon = privacy_engine.get_epsilon(delta=DELTA)\n",
    "    print(f\"Epoch {epoch+1}: LR: {lr:.6f}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"best_model_private.pth\")  \n",
    "\n",
    "print(f\"Best Test Accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7732d87-61da-4803-aaec-bf13997a401a",
   "metadata": {},
   "source": [
    "### Training and saving nonprivate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf5eba-bfe0-4eb3-9af5-545bab14ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 90  # Train for more epochs\n",
    "INITIAL_LR = 0.01  # Initial learning rate\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = convnet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=INITIAL_LR, weight_decay=1e-4)  # Use RMSprop with weight decay\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Calculate learning rate using cosine schedule\n",
    "    lr = INITIAL_LR * 0.5 * (1 + np.cos(np.pi * epoch / (EPOCHS + 1)))\n",
    "    \n",
    "    # Update the optimizer's learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: LR: {lr:.6f}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # Save the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"best_model_nonprivate.pth\")  #_RMSprop_Avepool\n",
    "\n",
    "print(f\"Best Test Accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25126239-6a0c-4dcf-b9ce-69ca4a00ce7d",
   "metadata": {},
   "source": [
    "## Conformal Prediction Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39b3ab-fbf1-4b86-8be8-0de61fa35255",
   "metadata": {},
   "source": [
    "### Helping functions for PCOSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be6f28-f385-4c31-ac8e-6a4211112c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRC(range_bounds, D, sigma):\n",
    "    \"\"\"\n",
    "    Noisy Range Count for float values with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    range_bounds (tuple): A tuple (a, b) representing the range [a, b].\n",
    "    D (list): The sorted dataset.\n",
    "    sigma (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    int: The noisy count of elements in the range [a, b].\n",
    "    \"\"\"\n",
    "    a, b = range_bounds\n",
    "    count = sum(1 for z in D if a <= z <= b)\n",
    "    noise = np.random.normal(0, sigma)\n",
    "    noisy_count = count + noise\n",
    "    return max(0, int(np.floor(noisy_count)))  # Ensure non-negative count\n",
    "\n",
    "def PrivQuant(D, alpha, rho, seed, lower_bound=0, upper_bound=1, delta=1e-10):\n",
    "    \"\"\"\n",
    "    Differentially Private Quantile Approximation Algorithm without integer conversion.\n",
    "\n",
    "    Parameters:\n",
    "    D (list): The sorted dataset.\n",
    "    alpha (float): The quantile level (e.g., 0.5 for median).\n",
    "    rho (float): The privacy parameter (smaller = more private).\n",
    "    lower_bound (float): Lower bound of the search space.\n",
    "    upper_bound (float): Upper bound of the search space.\n",
    "    delta (float): Small positive value to ensure convergence.\n",
    "\n",
    "    Returns:\n",
    "    float: A differentially private approximation of the quantile x_{(m)}.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n = len(D)\n",
    "    max_iterations = int(np.ceil(np.log2((upper_bound - lower_bound) / delta)))\n",
    "    sigma = np.sqrt(max_iterations / (2 * rho)) # Noise scale for Gaussian mechanism\n",
    "    m = int(np.ceil((1 - alpha) * (n + 1)))\n",
    "\n",
    "    left, right = lower_bound, upper_bound\n",
    "    random.seed(seed)\n",
    "    for i in range(max_iterations):\n",
    "        mid = (left + right) / 2\n",
    "        c = NoisyRC((lower_bound, mid), D, sigma)\n",
    "        \n",
    "        if c < m:\n",
    "            left = mid + delta\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return np.round((left + right) / 2, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a948789-a7be-4203-8853-fa4056afe692",
   "metadata": {},
   "source": [
    "### Helping functions for EXPONQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bbe758-da1c-43f9-9632-469fe0afd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qtilde(n,alpha,gamma,epsilon,m):\n",
    "    qtilde = (n+1)*(1-alpha)/(n*(1-gamma*alpha))+2/(epsilon*n)*np.log(m/(gamma*alpha))\n",
    "    qtilde = min(qtilde, 1-1e-12)\n",
    "    return qtilde\n",
    "\n",
    "def generate_scores(n):\n",
    "    return np.random.uniform(size=(n,))\n",
    "\n",
    "def hist_2_cdf(cumsum, bins, n):\n",
    "    def _cdf(t):\n",
    "        if t > bins[-2]:\n",
    "            return 1.0\n",
    "        elif t < bins[1]:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1-cumsum[np.searchsorted(bins, t)]/n\n",
    "    return _cdf\n",
    "\n",
    "def get_private_quantile(scores, alpha, epsilon, gamma, bins):\n",
    "    n = scores.shape[0]\n",
    "    epsilon_normed = epsilon*min(alpha, 1-alpha)\n",
    "    # Get the quantile\n",
    "    qtilde = get_qtilde(n, alpha, gamma, epsilon, bins.shape[0])\n",
    "    scores = scores.squeeze()\n",
    "    score_to_bin = np.digitize(scores,bins)\n",
    "    binned_scores = bins[np.minimum(score_to_bin,bins.shape[0]-1)]\n",
    "    w1 = np.digitize(binned_scores, bins)\n",
    "    w2 = np.digitize(binned_scores, bins, right=True)\n",
    "    # Clip bins\n",
    "    w1 = np.maximum(np.minimum(w1,bins.shape[0]-1),0)\n",
    "    w2 = np.maximum(np.minimum(w2,bins.shape[0]-1),0)\n",
    "    lower_mass = np.bincount(w1,minlength=bins.shape[0]).cumsum()/qtilde\n",
    "    upper_mass = (n-np.bincount(w2,minlength=bins.shape[0]).cumsum())/(1-qtilde)\n",
    "    w = np.maximum( lower_mass , upper_mass )\n",
    "    sampling_probabilities = softmax(-(epsilon_normed/2)*w)\n",
    "    # Check\n",
    "    sampling_probabilities = sampling_probabilities/sampling_probabilities.sum()\n",
    "    qhat = np.random.choice(bins,p=sampling_probabilities)\n",
    "    return qhat\n",
    "\n",
    "# Optimal gamma is a root.\n",
    "def get_optimal_gamma(scores,n,alpha,m,epsilon):\n",
    "    a = alpha**2\n",
    "    b = - ( alpha*epsilon*(n+1)*(1-alpha)/2 + 2*alpha )\n",
    "    c = 1\n",
    "    best_q = 1\n",
    "    gamma1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "    gamma2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "\n",
    "    gamma1 = min(max(gamma1,1e-12),1-1e-12)\n",
    "    gamma2 = min(max(gamma2,1e-12),1-1e-12)\n",
    "\n",
    "    bins = np.linspace(0,1,m)\n",
    "\n",
    "    q1 = get_private_quantile(scores, alpha, epsilon, gamma1, bins)\n",
    "    q2 = get_private_quantile(scores, alpha, epsilon, gamma2, bins)\n",
    "\n",
    "    return (gamma1, q1) if q1 < q2 else (gamma2, q2)\n",
    "\n",
    "def get_optimal_gamma_m(n, alpha, epsilon):\n",
    "    candidates_m = np.logspace(4,6,50).astype(int)\n",
    "    scores = np.random.rand(n,1)\n",
    "    best_m = int(1/alpha)\n",
    "    best_gamma = 1\n",
    "    best_q = 1\n",
    "    for m in candidates_m:\n",
    "        gamma, q = get_optimal_gamma(scores,n,alpha,m,epsilon)\n",
    "        if q < best_q:\n",
    "            best_q = q\n",
    "            best_m = m\n",
    "            best_gamma = gamma\n",
    "    return best_m, best_gamma\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n = 5000\n",
    "    alpha = 0.1\n",
    "    epsilon = 8 # removal definition, 5 is large.  usually we think of epsilon as 1 or 2.\n",
    "    mstar, gammastar = get_optimal_gamma_m(n, alpha, epsilon)\n",
    "    scores = np.random.random((n,))\n",
    "    q = get_private_quantile(scores, alpha, epsilon, gammastar, np.linspace(0,1,mstar))\n",
    "    print(f\"mstar: {mstar}, gammastar: {gammastar}, qhat: {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccfc2e-4c72-4361-9f42-4fca9a5c5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_conformal_scores(scores, labels):\n",
    "    conformal_scores = torch.tensor([scores[i,labels[i]] for i in range(scores.shape[0])])\n",
    "    return conformal_scores\n",
    "\n",
    "def get_shat_from_scores(scores, alpha):\n",
    "    return np.quantile(scores,1-alpha)\n",
    "\n",
    "def get_shat_from_scores_private(scores, alpha, epsilon, gamma, score_bins):\n",
    "    shat = get_private_quantile(scores, alpha, epsilon, gamma, score_bins)\n",
    "    return shat\n",
    "\n",
    "def platt_logits(calib_dataset, max_iters=10, lr=0.01, epsilon=0.01):\n",
    "    calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1024, shuffle=False, pin_memory=True)\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    nll_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    T = nn.Parameter(torch.Tensor([1.3]).to(device))\n",
    "\n",
    "    optimizer = optim.SGD([T], lr=lr)\n",
    "    for iter in range(max_iters):\n",
    "        T_old = T.item()\n",
    "        for x, targets in calib_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            x.requires_grad = True\n",
    "            out = x/T\n",
    "            loss = nll_criterion(out, targets.long().to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if abs(T_old - T.item()) < epsilon:\n",
    "            break\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b15ee-13e8-43b8-a984-ab91fd776597",
   "metadata": {},
   "source": [
    "### Helping functions for Lap-Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69242b63-214b-4a3c-a675-285e49c5dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_quantile_noisy_hist(x, q, epsilon, seed, bins=50, domain=(0.0, 1.0), rng=None):\n",
    "    \"\"\"\n",
    "    Differentially private quantile using a Laplace-noised histogram (ε-DP).\n",
    "\n",
    "    Args:\n",
    "        x (array-like): data vector (numeric).\n",
    "        q (float): desired quantile in (0,1).\n",
    "        epsilon (float): privacy budget for the entire histogram.\n",
    "        domain (tuple): (lo, hi) public bounds for clipping/binning.\n",
    "        bins (int): number of fixed, public bins.\n",
    "        rng: np.random.Generator (optional).\n",
    "\n",
    "    Returns:\n",
    "        float: DP quantile estimate (can lie between data points).\n",
    "\n",
    "    Privacy & assumptions:\n",
    "        - Data are clipped to the public domain (lo, hi).\n",
    "        - Build a fixed-bin histogram, add Lap(1/ε) noise to each bin count.\n",
    "        - Because each record contributes to exactly one bin, releasing\n",
    "          the full noisy histogram is ε-DP under add/remove adjacency.\n",
    "        - Quantile is computed from the noisy cumulative counts.\n",
    "\n",
    "    Notes:\n",
    "        - Works best if a reasonable public domain is known.\n",
    "        - For stability, negative noisy counts are floored at 0.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        raise ValueError(\"x must be non-empty.\")\n",
    "    if not (0 < q < 1):\n",
    "        raise ValueError(\"q must be in (0,1).\")\n",
    "    if epsilon <= 0:\n",
    "        raise ValueError(\"epsilon must be > 0.\")\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    lo, hi = domain\n",
    "    if not (lo < hi):\n",
    "        raise ValueError(\"domain must satisfy lo < hi.\")\n",
    "\n",
    "    # Clip to public domain\n",
    "    xc = np.clip(x, lo, hi)\n",
    "\n",
    "    # Fixed public bins\n",
    "    edges = np.linspace(lo, hi, bins + 1)\n",
    "    #print(f\"Bins: {edges}\")\n",
    "    counts, _ = np.histogram(xc, bins=edges)\n",
    "    #print(f\"Counts of histogram: {counts}\")\n",
    "\n",
    "    # Laplace noise to each bin (scale = 1/ε)\n",
    "    noise = rng.laplace(loc=0.0, scale=1.0/epsilon, size=bins)\n",
    "    #print(f\"Noise for each bin: {noise}\")\n",
    "    noisy = np.maximum(counts + noise, 0.0)\n",
    "    #print(noisy)\n",
    "\n",
    "    # Cumulative proportion\n",
    "    csum = np.cumsum(noisy)\n",
    "    if csum[-1] <= 0:\n",
    "        # extremely unlikely unless ε is tiny and n is tiny\n",
    "        return float(np.median(xc))\n",
    "\n",
    "    target = q * csum[-1]\n",
    "    j = np.searchsorted(csum, target)  # first bin reaching the target\n",
    "\n",
    "    j = int(np.clip(j, 0, bins - 1))\n",
    "    # Linear interpolation within the bin (simple, uniform-within-bin)\n",
    "    bin_lo, bin_hi = edges[j], edges[j + 1]\n",
    "    prev = csum[j - 1] if j > 0 else 0.0\n",
    "    within = (target - prev) / max(noisy[j], 1e-12)\n",
    "    within = np.clip(within, 0.0, 1.0)\n",
    "    return float(bin_lo + within * (bin_hi - bin_lo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4837d01-ce36-4bb9-8b7b-91b14fb6b9fb",
   "metadata": {},
   "source": [
    "## Helping function for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2cf15-1232-45c5-b968-bad9ff9e7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(conformal_scores, raw_scores, alpha, epsilon, gamma, score_bins, num_calib, seed, privateconformal):\n",
    "    total = conformal_scores.shape[0]\n",
    "    perm = torch.randperm(conformal_scores.shape[0])\n",
    "    conformal_scores = conformal_scores[perm]\n",
    "    raw_scores = raw_scores[perm]\n",
    "    calib_conformal_scores, val_conformal_scores = (1-conformal_scores[0:num_calib], 1-conformal_scores[num_calib:])\n",
    "    calib_raw_scores, val_raw_scores = (1-raw_scores[0:num_calib], 1-raw_scores[num_calib:])\n",
    "\n",
    "    # Always compute non-private results\n",
    "    threshold_nonpriv = get_shat_from_scores(calib_conformal_scores, alpha)\n",
    "    corrects_nonpriv = (val_conformal_scores < threshold_nonpriv)\n",
    "    sizes_nonpriv = (val_raw_scores < threshold_nonpriv).sum(dim=1)\n",
    "    \n",
    "    # Initialize private method results as None\n",
    "    corrects = corrects_PrivQuant = corrects_Lap_hist= None\n",
    "    sizes = sizes_PrivQuant = sizes_Lap_hist = None\n",
    "    shat = threshold_PrivQuant = threshold_Lap_hist=  None\n",
    "\n",
    "    if privateconformal:\n",
    "        shat = get_shat_from_scores_private(calib_conformal_scores, alpha, epsilon, gamma, score_bins)\n",
    "        epsilon_comform = (epsilon**2)/2\n",
    "        threshold_PrivQuant = PrivQuant(calib_conformal_scores, alpha, epsilon_comform, seed, lower_bound=0, upper_bound=1, delta=1e-10)\n",
    "        q =1-alpha\n",
    "        threshold_Lap_hist = dp_quantile_noisy_hist(calib_conformal_scores, q, epsilon, seed) \n",
    "        \n",
    "        corrects = (val_conformal_scores < shat)\n",
    "        corrects_PrivQuant = (val_conformal_scores < threshold_PrivQuant)\n",
    "        corrects_Lap_hist = (val_conformal_scores < threshold_Lap_hist)\n",
    "        sizes = (val_raw_scores < shat).sum(dim=1)\n",
    "        sizes_PrivQuant = (val_raw_scores < threshold_PrivQuant).sum(dim=1)\n",
    "        sizes_Lap_hist = (val_raw_scores < threshold_Lap_hist).sum(dim=1)\n",
    "\n",
    "    return (\n",
    "        corrects.float().mean().item() if corrects is not None else np.nan,\n",
    "        corrects_PrivQuant.float().mean().item() if corrects_PrivQuant is not None else np.nan,\n",
    "        corrects_Lap_hist.float().mean().item() if corrects_Lap_hist is not None else np.nan,\n",
    "        corrects_nonpriv.float().mean().item(),\n",
    "        sizes if sizes is not None else torch.tensor([]),\n",
    "        sizes_PrivQuant if sizes_PrivQuant is not None else torch.tensor([]),\n",
    "        sizes_Lap_hist if sizes_Lap_hist is not None else torch.tensor([]),\n",
    "        sizes_nonpriv,\n",
    "        shat if shat is not None else np.nan,\n",
    "        threshold_PrivQuant if threshold_PrivQuant is not None else np.nan,\n",
    "        threshold_Lap_hist if threshold_Lap_hist is not None else np.nan,\n",
    "        threshold_nonpriv\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86e93f-8696-4f00-9cc7-61126d221428",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e8a84-9a9f-4d59-bc9d-3e4fbe5bbedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the root directory\n",
    "dirname = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(private=True):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = convnet(num_classes=10).to(device)\n",
    "\n",
    "    if private:\n",
    "        model_path = \".../best_model_private.pth\"  #TO DO: Put the complete path to the saved trained private model\n",
    "    else:\n",
    "        model_path = \".../best_model_nonprivate.pth\"  #TO DO: Put the complete path to the saved trained non-private model\n",
    "\n",
    "\n",
    "\n",
    "    # Load the model state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Remove the \"_module.\" prefix from keys if present\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_module.\"):\n",
    "            name = k[8:]  # Remove \"_module.\" prefix\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the modified state dict\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute logits and targets\n",
    "def get_logits_targets(model, loader):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logits = torch.zeros((len(loader.dataset), 10))  # 10 classes in CIFAR\n",
    "    labels = torch.zeros((len(loader.dataset),))\n",
    "    model = model.to(device)\n",
    "    i = 0\n",
    "    print('Computing logits for model (only happens once).')\n",
    "    with torch.no_grad():\n",
    "        for x, targets in tqdm(loader):\n",
    "            batch_logits = model(x.to(device)).detach().cpu()\n",
    "            logits[i:(i + x.shape[0]), :] = batch_logits\n",
    "            labels[i:(i + x.shape[0])] = targets.cpu()\n",
    "            i = i + x.shape[0]\n",
    "\n",
    "    # Construct the dataset\n",
    "    dataset_logits = torch.utils.data.TensorDataset(logits, labels.long())\n",
    "    return dataset_logits\n",
    "\n",
    "# Load logits dataset\n",
    "def get_logits_dataset(private, datasetname, datasetpath, cache=os.path.join(dirname, '.cache/')):\n",
    "    fname = os.path.join(cache, datasetname, 'private.pkl' if private else 'nonprivate.pkl')\n",
    "\n",
    "    # If the file exists, load and return it\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as handle:\n",
    "            return pkl.load(handle)\n",
    "\n",
    "    # Else, load the model, run it on the dataset, and save/return the output\n",
    "    model = get_model(private)\n",
    "\n",
    "    test_transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    test_dataset = CIFAR10(root=datasetpath, train=False, download=True, transform=test_transform)\n",
    "\n",
    "    loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Get the logits and targets\n",
    "    dataset_logits = get_logits_targets(model, loader)\n",
    "\n",
    "    # Save the dataset\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pkl.dump(dataset_logits, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dataset_logits\n",
    "\n",
    "# Fix randomness for reproducibility\n",
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Get CIFAR-10 class names\n",
    "def get_cifar10_classes():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Experiment function\n",
    "def experiment(alpha, epsilon, gamma, num_calib, m, seed, cifar10_root, privatemodel, privateconformal):\n",
    "    df_list = []\n",
    "    score_bins = np.linspace(0, 1, m)\n",
    "    fname = f'.cache/opt_{privatemodel}_{privateconformal}_{alpha}_{epsilon}_{num_calib}_{m}bins_dataframe.pkl'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(fname)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    dataset_precomputed = get_logits_dataset(privatemodel, 'CIFAR10', cifar10_root)\n",
    "    print('Dataset loaded')\n",
    "    T = platt_logits(dataset_precomputed)\n",
    "    logits, labels = dataset_precomputed.tensors\n",
    "    scores = (logits / T.cpu()).softmax(dim=1)\n",
    "\n",
    "    accuracy = (scores.argmax(dim=1) == labels).float().mean()\n",
    "    print(f\"Private model: {privatemodel}, Accuracy: {accuracy}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        conformal_scores = get_conformal_scores(scores, labels)\n",
    "        local_df_list = []\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            seed = i+seed\n",
    "            cvg1, cvg2, cvg3, cvg4, szs1, szs2, szs3, szs4, shat, threshold_PrivQuant, threshold_Lap_hist, threshold_nonpriv = trial_precomputed(\n",
    "                conformal_scores, scores, alpha, epsilon, gamma, score_bins, num_calib, seed, privateconformal)\n",
    "            \n",
    "            # Store results with consistent structure\n",
    "            dict_local = {\n",
    "                \"NonprivQuant\": cvg4,\n",
    "                \"sizes_NonprivQuant\": [szs4],\n",
    "                \"Anas et. al\": cvg1 if privateconformal else np.nan,\n",
    "                \"PrivQuant\": cvg2 if privateconformal else np.nan,\n",
    "                \"Lap_hist\": cvg3 if privateconformal else np.nan,\n",
    "                \"sizes_Anas et. al\": [szs1] if privateconformal and szs1 is not None else [torch.tensor([])],\n",
    "                \"sizes_PrivQuant\": [szs2] if privateconformal and szs2 is not None else [torch.tensor([])],\n",
    "                \"sizes_Lap_hist\": [szs3] if privateconformal and szs3 is not None else [torch.tensor([])],\n",
    "                \"$\\\\hat{s}$\": shat if privateconformal else np.nan,\n",
    "                \"$\\\\hat{q}_$PrivQuant\": threshold_PrivQuant if privateconformal else np.nan,\n",
    "                \"Lap_hist_thrs\": threshold_Lap_hist if privateconformal else np.nan,\n",
    "                \"$\\\\alpha$\": alpha,\n",
    "                \"$\\\\epsilon$\": epsilon,\n",
    "                \"PrivateConformal\": privateconformal,  \n",
    "                \"PrivateModel\": privatemodel          \n",
    "    }\n",
    "    \n",
    "            \n",
    "            df_local = pd.DataFrame(dict_local)\n",
    "            local_df_list.append(df_local)\n",
    "        \n",
    "        df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "        os.makedirs('.cache', exist_ok=True)\n",
    "        df.to_pickle(fname)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "shutil.rmtree('.cache', ignore_errors=True)\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set(palette='pastel', font='serif')\n",
    "    sns.set_style('white')\n",
    "    fix_randomness(seed=0)\n",
    "\n",
    "    save_path = 'df_list_CIFAR_10_results.pkl'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            df_list = pkl.load(f)\n",
    "    else:\n",
    "        cifar10_root = './data/cifar10'\n",
    "        privateconformals = [False, True]\n",
    "        privatemodels = [False, True]\n",
    "\n",
    "        alpha = 0.1\n",
    "        epsilon = 1\n",
    "        num_calib = 5000\n",
    "        num_trials = 1000\n",
    "        seed = 123\n",
    "        mstar, gammastar = get_optimal_gamma_m(num_calib, alpha, epsilon)\n",
    "\n",
    "        df_list = []\n",
    "        for privateconformal in privateconformals:\n",
    "            for privatemodel in privatemodels:\n",
    "                df_list.append(\n",
    "                    experiment(alpha, epsilon, gammastar, num_calib, mstar, seed,\n",
    "                               cifar10_root=cifar10_root, \n",
    "                               privatemodel=privatemodel,\n",
    "                               privateconformal=privateconformal)\n",
    "                )\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a58a2-2791-405a-90d1-2b334c08f973",
   "metadata": {},
   "source": [
    "### Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7686084-a10d-400f-8678-249b553980cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'wb') as f:\n",
    "            pkl.dump(df_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a08948-0640-46ae-9dff-3f26e8ef2850",
   "metadata": {},
   "source": [
    "### Processing and saving results in files for ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27761db-c88f-4d53-ace4-222611b76f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sizes_to_averages(size_series, trials=1000, eval_points=5000):\n",
    "    \"\"\"Process size series to compute average set size per trial\"\"\"\n",
    "    # First convert all elements to float values (handling tensors if present)\n",
    "    processed_sizes = []\n",
    "    for val in size_series.explode().dropna():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            processed_sizes.append(float(val.item()))\n",
    "        else:\n",
    "            processed_sizes.append(float(val))\n",
    "    \n",
    "    if not processed_sizes:\n",
    "        print(\"Warning: No valid size data found\")\n",
    "        return np.array([])\n",
    "    \n",
    "    # Check if we have enough data points\n",
    "    expected_points = trials * eval_points\n",
    "    if len(processed_sizes) != expected_points:\n",
    "        print(f\"Warning: Expected {expected_points} size points, got {len(processed_sizes)}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    # Reshape and compute averages\n",
    "    try:\n",
    "        size_array = np.array(processed_sizes).reshape(trials, eval_points)\n",
    "        return np.mean(size_array, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing averages: {str(e)}\")\n",
    "        return np.array([])\n",
    "\n",
    "def safe_to_dataframe(data_dict):\n",
    "    \"\"\"Convert dictionary to DataFrame, handling unequal lengths and empty data\"\"\"\n",
    "    if not data_dict:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter out empty arrays\n",
    "    filtered = {k: v for k, v in data_dict.items() if len(v) > 0}\n",
    "    if not filtered:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    max_len = max(len(v) for v in filtered.values())\n",
    "    padded = {k: np.pad(v, (0, max_len - len(v)), \n",
    "             mode='constant', constant_values=np.nan)\n",
    "             for k, v in filtered.items()}\n",
    "    return pd.DataFrame(padded)\n",
    "\n",
    "def main():\n",
    "    # Load your data\n",
    "    try:\n",
    "        with open('df_list_CIFAR_10_results.pkl', 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found\")\n",
    "        return\n",
    "    except pickle.PickleError:\n",
    "        print(\"Error: Could not unpickle the file\")\n",
    "        return\n",
    "\n",
    "    # Setting names mapping\n",
    "    setting_names = {\n",
    "        (False, False): \"NonPrivateModel_NonPrivateConformal\",\n",
    "        (False, True): \"NonPrivateModel_PrivateConformal\",\n",
    "        (True, False): \"PrivateModel_NonPrivateConformal\",\n",
    "        (True, True): \"PrivateModel_PrivateConformal\"\n",
    "    }\n",
    "\n",
    "    # Initialize storage\n",
    "    results = {\n",
    "        'coverage': {setting: {} for setting in setting_names.values()},\n",
    "        'size_avg': {setting: {} for setting in setting_names.values()}\n",
    "    }\n",
    "\n",
    "    # Process each setting's DataFrame\n",
    "    for df_idx, df in enumerate(df_list):\n",
    "        try:\n",
    "            private_model = df[\"PrivateModel\"].iloc[0]\n",
    "            private_conformal = df[\"PrivateConformal\"].iloc[0]\n",
    "            setting = setting_names[(private_model, private_conformal)]\n",
    "            \n",
    "            # Coverage data\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\", \"Lap_hist\"]:\n",
    "                if method in df.columns:\n",
    "                    cov_data = df[method].dropna().astype(float).values\n",
    "                    results['coverage'][setting][method] = cov_data\n",
    "            \n",
    "            # Size data - computing averages per trial\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\", \"Lap_hist\"]:\n",
    "                size_key = f\"sizes_{method}\"\n",
    "                if size_key in df.columns:\n",
    "                    avg_sizes = process_sizes_to_averages(df[size_key])\n",
    "                    if len(avg_sizes) > 0:  # Only store if we got valid averages\n",
    "                        results['size_avg'][setting][method] = avg_sizes\n",
    "                    else:\n",
    "                        print(f\"No valid size data for {method} in setting {setting} (DF #{df_idx+1})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataframe #{df_idx+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save coverage data\n",
    "    for setting in setting_names.values():\n",
    "        if results['coverage'][setting]:\n",
    "            df = safe_to_dataframe(results['coverage'][setting])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'coverage_{setting}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No coverage data to save for {setting}\")\n",
    "\n",
    "    # Save average size data\n",
    "    for setting in setting_names.values():\n",
    "        if results['size_avg'][setting]:\n",
    "            df = safe_to_dataframe(results['size_avg'][setting])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'avg_size_{setting}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No average size data to save for {setting}\")\n",
    "\n",
    "    print(\"Data processing complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
